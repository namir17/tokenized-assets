{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e09d98a9-d0f3-42dc-9d32-f4a263c5b528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "def round_to_next_minute(date_str):\n",
    "    # convert to datetime object\n",
    "    dt = datetime.datetime.fromisoformat(date_str)\n",
    "\n",
    "    # check if minute has already begun\n",
    "    if dt.second > 0:\n",
    "        # add the remaining seconds to round up to next minute\n",
    "        dt += datetime.timedelta(seconds=(60 - dt.second))\n",
    "\n",
    "    # set seconds to zero\n",
    "    dt = dt.replace(second=0, microsecond=0)\n",
    "\n",
    "    # convert to desired format\n",
    "    new_date_str = dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    return new_date_str\n",
    "\n",
    "def read_csv_round_dates(file_path):\n",
    "    # read CSV file into Pandas dataframe\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # round date column to next minute\n",
    "    df['datetime'] = df['time'].apply(round_to_next_minute)\n",
    "\n",
    "    # convert to desired format\n",
    "    df['datetime'] = pd.to_datetime(df['datetime']).dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_ftx_data(file_path):\n",
    "    # read and process FTX data\n",
    "    ftx_df = read_csv_round_dates(file_path)\n",
    "    \n",
    "    # Convert the 'date' column to a pandas datetime object\n",
    "    ftx_df['datetime'] = pd.to_datetime(ftx_df['datetime'])\n",
    "\n",
    "    # Extract the day from the datetime object\n",
    "    ftx_df['day'] = ftx_df['datetime'].dt.date\n",
    "    \n",
    "    ftx_df[\"price*size\"] = ftx_df[\"price\"]*ftx_df[\"size\"]\n",
    "\n",
    "    ftx_df['side'] = ftx_df['side'].replace({'buy': 1, 'sell': -1})\n",
    "    ftx_df['adjusted_size'] = ftx_df['side'] * ftx_df['size']\n",
    "\n",
    "    return ftx_df\n",
    "\n",
    "def process_intraday_data(file_path):\n",
    "    # read and process intraday data\n",
    "    intra_df = pd.read_csv(file_path)\n",
    "\n",
    "    intra_df[\"average\"] = (intra_df[\"open\"] + intra_df[\"high\"] + intra_df[\"low\"] + intra_df[\"close\"]) / 4\n",
    "\n",
    "    intra_df['datetime'] = pd.to_datetime(intra_df['datetime'])\n",
    "    intra_df['day'] = intra_df['datetime'].dt.date\n",
    "\n",
    "    return intra_df\n",
    "\n",
    "def fill_missing_values(df):\n",
    "    # fill missing values in dataframe\n",
    "    df['average'].fillna(method='ffill', inplace=True)\n",
    "    df['average'].interpolate(method='linear', inplace=True)\n",
    "    df['average'].fillna(method='ffill', inplace=True)\n",
    "    df['average'].fillna(method='bfill', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def merge_dataframes(ftx_df, intra_df):\n",
    "    merged_df = pd.merge(ftx_df, intra_df[['datetime', 'average']], on='datetime', how='left')\n",
    "    fill_missing_values(merged_df)\n",
    "    merged_df[\"premium\"] = (merged_df[\"price\"] / merged_df[\"average\"]) - 1\n",
    "    merged_df[\"premium*size\"] = merged_df[\"premium\"] * merged_df[\"size\"]\n",
    "     \n",
    "    return merged_df\n",
    "\n",
    "#def merge_dataframes(ftx_df, intra_df):\n",
    "# merge FTX and intraday dataframes\n",
    "#    merged_df = pd.merge(ftx_df, intra_df[['day', 'average']], on='day', how='left')\n",
    "#   merged_df[\"premium\"] = (merged_df[\"price\"] / merged_df[\"average\"]) - 1\n",
    "    \n",
    "# separate unmatched rows into two dataframes\n",
    "#  ftx_unmatched = ftx_df.loc[~ftx_df.index.isin(ftx_df.index)]\n",
    "# intra_unmatched = intra_df.loc[~intra_df.index.isin(intra_df.index)]\n",
    "\n",
    "#return merged_df, ftx_unmatched, intra_unmatched\n",
    "\n",
    "\n",
    "def create_daily_data(merged_df, ftx_df):\n",
    "    daily_merged_df = merged_df.groupby('day').agg({'price':'mean', 'size':'sum', 'premium':'mean', 'premium*size':'mean', 'price*size':'sum', 'average':'mean', \"adjusted_size\":\"sum\"})\n",
    "    daily_merged_df[\"weighted_price\"] = daily_merged_df[\"price*size\"] / daily_merged_df[\"size\"]\n",
    "    daily_merged_df[\"weighted_average_premium\"] = daily_merged_df[\"premium*size\"] / daily_merged_df[\"size\"]\n",
    "    daily_merged_df['count'] = ftx_df.groupby('day')['id'].count()\n",
    "    \n",
    "    buy_df = ftx_df[ftx_df['side']== 1]\n",
    "    sell_df = ftx_df[ftx_df['side']== -1]\n",
    "    \n",
    "    daily_merged_df['WAP_buy'] = buy_df.groupby('day').apply(lambda x: (x['price']*x['size']).sum() / x['size'].sum())\n",
    "    daily_merged_df['WAP_sell'] = sell_df.groupby('day').apply(lambda x: (x['price']*x['size']).sum() / x['size'].sum())\n",
    "\n",
    "    # Add 'day' column back as a regular column\n",
    "    daily_merged_df.reset_index(inplace=True)\n",
    "    \n",
    "    return daily_merged_df\n",
    "    \n",
    "def process_daily_data(file_path):\n",
    "    # read and process daily data\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    df[\"average\"] = (df[\"OPEN\"] + df[\"HIGH\"] + df[\"LOW\"]+ df[\"CLOSE\"]) / 4\n",
    "\n",
    "    df[\"abs_trade_size\"] = df[\"VOLUME\"] * df[\"average\"]\n",
    "\n",
    "    df[\"avg_trade_size\"] = df[\"average\"] * df[\"VOLUME\"] / df[\"COUNT\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def plot_data(df):\n",
    "    # plot the data\n",
    "    df.plot(x=\"datetime\", y=['average', 'price'], kind='line')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a940632d-f427-4f6c-9c4d-3f4df688685b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "# Define function to process data for a given stock symbol\n",
    "def process_data(stock_symbol):\n",
    "    # Define file paths for FTX, intraday and daily data\n",
    "    ftx_file_path = f\"/Users/namirsacic/tokenized-assets/ftx/ftx_{stock_symbol}USD.csv\"\n",
    "    intra_file_path = f\"/Users/namirsacic/tokenized-assets/intraday/{stock_symbol}_US_data.csv\"\n",
    "    \n",
    "    daily_file_path = glob.glob(f\"/Users/namirsacic/tokenized-assets/daily_data/{stock_symbol}.*.csv\")[0]\n",
    "\n",
    "    # Process FTX data\n",
    "    ftx_df = process_ftx_data(ftx_file_path)\n",
    "\n",
    "    # Process intraday data\n",
    "    intra_df = process_intraday_data(intra_file_path)\n",
    "    \n",
    "    #Merge dataframes\n",
    "    merged_df = merge_dataframes(ftx_df, intra_df)\n",
    "\n",
    "    # Create daily data\n",
    "    daily_merged_df = create_daily_data(merged_df, ftx_df)\n",
    "\n",
    "    # Process daily data\n",
    "    daily_df = process_daily_data(daily_file_path)\n",
    "\n",
    "    #Fill missing values\n",
    "    #daily_df = fill_missing_values(daily_df)\n",
    "\n",
    "    # Plot data\n",
    "    #plot_data(daily_df)\n",
    "\n",
    "    # Store dataframes as csv files\n",
    "    \n",
    "    #merged_df.to_csv(f\"/Users/namirsacic/tokenized-assets/data_analysis/processed_data/merged_data/{stock_symbol}_merged.csv\", index=False)\n",
    "    daily_merged_df.to_csv(f\"/Users/namirsacic/tokenized-assets/data_analysis/processed_data/daily_merged_data/{stock_symbol}_daily_merged.csv\", index=False)\n",
    "    #daily_df.to_csv(f\"/Users/namirsacic/tokenized-assets/data_analysis/processed_data/daily_data/{stock_symbol}_daily.csv\", index=False)\n",
    "    #ftx_unmatched.to_csv(f\"/Users/namirsacic/Desktop/tokenized-assets/data_analysis/processed_data/ftx_unmatched/{stock_symbol}_ftx_unmatched.csv\", index=False)\n",
    "    #intra_unmatched.to_csv(f\"/Users/namirsacic/Desktop/tokenized-assets/data_analysis/processed_data/intraday_unmatched/{stock_symbol}_intra_unmatched.csv\", index=False)\n",
    "\n",
    "# List of stock symbols to process\n",
    "#stock_symbols = [\"AAPL\",\"ABNB\" \"ACB\", \"AMC\" \"AMZN\", \"APHA\", \"ARKK\", \"BABA\", \"BB\", \"BILI\", \"BITO\", \"BNTX\",\n",
    "#                 \"BYND\", \"CGC\", \"COIN\", \"CRON\", \"DKNG\", \"GDX\", \"GDXJ\", \"GLD\", \"GME\", \"GOOGL\", \n",
    "#                 \"HOOD\", \"MRNA\", \"MSFT\", \"MSTR\", \"NFLX\", \"NIO\", \"NOK\", \"NVDA\", \"PENN\", \"PFE\", \"PYPL\", \n",
    "#                 \"SPY\", \"SLV\", \"SQ\", \"TLRY\", \"TSLA\", \"TWTR\", \"UBER\", \"USO\", \"ZM\"]\n",
    "                \n",
    "stock_symbols = [\"AAPL\", \"ABNB\", \"ACB\", \"AMC\", \"AMZN\", \"ARKK\", \"BABA\", \"BB\", \"BITO\", \"BNTX\",\n",
    "               \"BYND\", \"COIN\", \"CRON\", \"DKNG\", \"GDX\", \"GDXJ\", \"GLD\", \"GME\", \"GOOGL\", \n",
    "                \"HOOD\", \"MRNA\", \"MSTR\", \"NFLX\", \"NIO\", \"NVDA\", \"PENN\", \"PFE\", \"PYPL\", \n",
    "                \"SPY\", \"SLV\", \"SQ\", \"TLRY\", \"TSLA\", \"UBER\", \"USO\", \"ZM\"]\n",
    "\n",
    "\n",
    "\n",
    "#Missing daily data: APHA, BILI, CGC, MSFT, NOK\n",
    "#Missing intraday: AMD\n",
    "\n",
    "# Process data for each stock symbol\n",
    "for stock_symbol in stock_symbols:\n",
    "    process_data(stock_symbol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba92c89-6050-490e-993d-29486b5b0660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4044a0c6-e407-405b-a330-2f4e1cf221cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
