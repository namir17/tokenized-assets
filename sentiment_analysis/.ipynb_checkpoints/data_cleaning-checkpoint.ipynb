{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/namirsacic/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/namirsacic/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from path import Path\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read a CSV file and return a Pandas dataframe\n",
    "def csv_to_df(csv_file):\n",
    "    file_path = Path(csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter for English tweets\n",
    "def filter_tweets(df):\n",
    "    df = df[df.lang == \"en\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop rows that don't contain tweets\n",
    "def drop_rows(df): \n",
    "    index_names = df[ df['created_at'] == \"created_at\" ].index\n",
    "  \n",
    "    # drop these row indexes\n",
    "    # from dataFrame\n",
    "    df.drop(index_names, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of needed columns\n",
    "needed_columns = [\"created_at\", \"tweet\"]\n",
    "\n",
    "# Function to get a Pandas series of tweets\n",
    "def get_series_of_tweets(df):\n",
    "    \n",
    "    df = df[needed_columns]\n",
    "    df.created_at = pd.to_datetime(df.created_at).dt.date\n",
    "    tweets = df[\"tweet\"]\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following function removes URLs, punctuation, stopwords and converts text into lowercase\n",
    "def clean_tweets(tweets):\n",
    "    #Removing URLs from tweets\n",
    "    remove_url = lambda x: re.sub(r'https\\S+', '', str(x))\n",
    "    tweets_lr = tweets.apply(remove_url)\n",
    "    tweets_lr\n",
    "    \n",
    "    #Convert to lowercase\n",
    "    to_lower = lambda x: x.lower()\n",
    "    tweets_lr_lc = tweets_lr.apply(to_lower)\n",
    "    tweets_lr_lc\n",
    "    \n",
    "    #Removing punctuation\n",
    "    remove_puncs = lambda x: x.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    tweets_lr_lc_np = tweets_lr_lc.apply(remove_puncs)\n",
    "    tweets_lr_lc_np\n",
    "    \n",
    "    #Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"English\"))\n",
    "\n",
    "    remove_words = lambda x: \" \".join([word for word in x.split() if word not in stop_words])\n",
    "    tweets_lr_lc_np_ns = tweets_lr_lc_np.apply(remove_words)\n",
    "    \n",
    "    return tweets_lr_lc_np_ns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(df, tweets, csv_name):\n",
    "    df.tweet = tweets\n",
    "    df.to_csv(csv_name, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/zbjsnt8j5sn16bwvmsxbqhmr0000gn/T/ipykernel_73196/51829738.py:3: DtypeWarning: Columns (0,3,5,6,7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "twr_df = csv_to_df(\"/Users/namirsacic/tokenized-assets/sentiment_analysis/twr_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "twr_df = filter_tweets(twr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "twr_df = drop_rows(twr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/zbjsnt8j5sn16bwvmsxbqhmr0000gn/T/ipykernel_73196/3043651891.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.created_at = pd.to_datetime(df.created_at).dt.date\n"
     ]
    }
   ],
   "source": [
    "twr_tweets = get_series_of_tweets(twr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "twr_tweets = clean_tweets(twr_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv(twr_df, twr_tweets, \"twr_cleaned.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
